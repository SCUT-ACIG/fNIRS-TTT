{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26786174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/U_PZL2022KF0005/home/luowanxiang/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from http.client import ImproperConnectionState\n",
    "import torch\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from model import fNIRS_T, fNIRS_PreT\n",
    "from dataloader import Dataset, Load_Dataset_A, Load_Dataset_B, Load_Dataset_C\n",
    "import os\n",
    "import argparse\n",
    "from model_init import init_model_MK2\n",
    "parser = argparse.ArgumentParser(description=\"传入您的device_id, dataset_id(0->A, 1->B, 2->C), models_id(0->T, 1->PreT, 2->Ours)\")\n",
    "parser.add_argument('--device_id', type=str, default='0', help='传入您的device_id')\n",
    "# parser.add_argument('--dataset_id', type=int, default='1', help='传入您的dataset_id(0->A, 1->B, 2->C)')\n",
    "parser.add_argument('--models_id', type=int, default='16', help='传入您的models_id(0->T, 1->PreT, 2->Ours)')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "def average_log(save_path):\n",
    "    # 存储每个epoch的总和，用于求平均\n",
    "    train_loss_all = []\n",
    "    train_acc_all = []\n",
    "    test_loss_all = []\n",
    "    test_acc_all = []\n",
    "\n",
    "    sub_folders = [f.path for f in os.scandir(save_path) if f.is_dir()]  # 获取所有子文件夹\n",
    "    num_subs = len(sub_folders)  # 记录子文件夹数量\n",
    "\n",
    "    # 遍历所有子文件夹，读取train_loss_history.txt, train_acc_history.txt等文件\n",
    "    for sub_folder in sub_folders:\n",
    "        with open(os.path.join(sub_folder, 'train_loss_history.txt'), 'r') as f:\n",
    "            train_loss_history = eval(f.read())  # 假设文件内容是一个列表\n",
    "        \n",
    "        with open(os.path.join(sub_folder, 'train_acc_history.txt'), 'r') as f:\n",
    "            train_acc_history = eval(f.read())\n",
    "        \n",
    "        with open(os.path.join(sub_folder, 'test_loss_history.txt'), 'r') as f:\n",
    "            test_loss_history = eval(f.read())\n",
    "        \n",
    "        with open(os.path.join(sub_folder, 'test_acc_history.txt'), 'r') as f:\n",
    "            test_acc_history = eval(f.read())\n",
    "\n",
    "        # 将当前子文件夹的数据累加到对应的总和列表中\n",
    "        if len(train_loss_all) == 0:  # 第一次初始化每个列表\n",
    "            train_loss_all = np.array(train_loss_history)\n",
    "            train_acc_all = np.array(train_acc_history)\n",
    "            test_loss_all = np.array(test_loss_history)\n",
    "            test_acc_all = np.array(test_acc_history)\n",
    "        else:\n",
    "            train_loss_all += np.array(train_loss_history)\n",
    "            train_acc_all += np.array(train_acc_history)\n",
    "            test_loss_all += np.array(test_loss_history)\n",
    "            test_acc_all += np.array(test_acc_history)\n",
    "\n",
    "        # 计算平均值，并保留三位小数\n",
    "        train_loss_avg = [round(loss, 3) for loss in (train_loss_all / num_subs).tolist()]\n",
    "        train_acc_avg = [round(acc, 3) for acc in (train_acc_all / num_subs).tolist()]\n",
    "        test_loss_avg = [round(loss, 3) for loss in (test_loss_all / num_subs).tolist()]\n",
    "        test_acc_avg = [round(acc, 3) for acc in (test_acc_all / num_subs).tolist()]\n",
    "\n",
    "    # 将最终结果保存到文件\n",
    "    with open(os.path.join(save_path, 'final_train_loss_history.txt'), 'w') as f:\n",
    "        f.write(str(train_loss_avg))\n",
    "    \n",
    "    with open(os.path.join(save_path, 'final_train_acc_history.txt'), 'w') as f:\n",
    "        f.write(str(train_acc_avg))\n",
    "    \n",
    "    with open(os.path.join(save_path, 'final_test_loss_history.txt'), 'w') as f:\n",
    "        f.write(str(test_loss_avg))\n",
    "    \n",
    "    with open(os.path.join(save_path, 'final_test_acc_history.txt'), 'w') as f:\n",
    "        f.write(str(test_acc_avg))\n",
    "\n",
    "\n",
    "class LabelSmoothing(torch.nn.Module):\n",
    "    \"\"\"NLL loss with label smoothing.\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d59c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Net\n",
      "C\n",
      "1  OK\n",
      "2  OK\n",
      "3  OK\n",
      "4  OK\n",
      "5  OK\n",
      "6  OK\n",
      "7  OK\n",
      "8  OK\n",
      "9  OK\n",
      "10  OK\n",
      "11  OK\n",
      "12  OK\n",
      "13  OK\n",
      "14  OK\n",
      "15  OK\n",
      "16  OK\n",
      "17  OK\n",
      "18  OK\n",
      "19  OK\n",
      "20  OK\n",
      "21  OK\n",
      "22  OK\n",
      "23  OK\n",
      "24  OK\n",
      "25  OK\n",
      "26  OK\n",
      "27  OK\n",
      "28  OK\n",
      "29  OK\n",
      "30  OK\n",
      "feature  (2250, 2, 20, 256)\n",
      "label  (2250,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training epochs\n",
    "EPOCH = 1\n",
    "\n",
    "# Device setting\n",
    "\n",
    "device_id = args.device_id\n",
    "\n",
    "# bs\n",
    "batch_size = 128\n",
    "\n",
    "# Select dataset\n",
    "dataset = ['C']\n",
    "\n",
    "models = ['fNIRS-T', 'fNIRS-PreT',  'CT-Net', 'fNIRS_TTT_LM', 'fNIRS_TTT_M', 'fNIRS_TTT_LL', 'fNIRS_TTT_L']\n",
    "models_id = 2\n",
    "print(models[models_id])\n",
    "\n",
    "for index, ds in enumerate(dataset):\n",
    "    dataset_id = index\n",
    "\n",
    "    print(dataset[dataset_id])\n",
    "\n",
    "\n",
    "    if dataset[dataset_id] == 'A':\n",
    "        flooding_level = [0, 0, 0]\n",
    "        if models[models_id] == 'fNIRS-T' or 1:\n",
    "            feature, label = Load_Dataset_A(\"data/A\", model='fNIRS-T')\n",
    "        elif models[models_id] == 'fNIRS-PreT':\n",
    "            feature, label = Load_Dataset_A(\"data/A\", model='fNIRS-PreT')\n",
    "    elif dataset[dataset_id] == 'B':\n",
    "        if models[models_id] == 'fNIRS-T' or 1:\n",
    "            flooding_level = [0.45, 0.40, 0.35]\n",
    "        else:\n",
    "            flooding_level = [0.40, 0.38, 0.35]\n",
    "        feature, label = Load_Dataset_B(\"data/B\")\n",
    "    elif dataset[dataset_id] == 'C':\n",
    "        flooding_level = [0.45, 0.40, 0.35]\n",
    "        feature, label = Load_Dataset_C(\"data/C\")\n",
    "    \n",
    "    _, _, channels, sampling_points = feature.shape\n",
    "\n",
    "    feature = feature.reshape((label.shape[0], -1))\n",
    "    # 5 × 5-fold-CV\n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    n_runs = 0\n",
    "\n",
    "    result_acc = []\n",
    "    result_pre = []\n",
    "    result_rec = []\n",
    "    result_f1  = []\n",
    "    result_kap = []\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af112b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/U_PZL2022KF0005/home/luowanxiang/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpus = [0]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import ttt\n",
    "from pandas import ExcelWriter\n",
    "# from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "# from utils import calMetrics\n",
    "# from utils import calculatePerClass\n",
    "# from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from utils import numberClassChannel\n",
    "# from utils import load_data_evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124ea8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import false, true\n",
    "\n",
    "\n",
    "class _ScaleModule(nn.Module):\n",
    "    def __init__(self, dims, init_scale=1.0, init_bias=0):\n",
    "        super(_ScaleModule, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.weight = nn.Parameter(torch.ones(*dims) * init_scale)\n",
    "        self.bias = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mul(self.weight, x)\n",
    "\n",
    "class WTConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, bias=True, wt_levels=1, wt_type='db1'):\n",
    "        super(WTConv2d, self).__init__()\n",
    "        # super().__init__()\n",
    "        # assert in_channels == out_channels\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.wt_levels = wt_levels\n",
    "        self.stride = stride\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.wt_filter, self.iwt_filter = wavelet.create_wavelet_filter(wt_type, in_channels, in_channels, torch.float)\n",
    "        self.wt_filter = nn.Parameter(self.wt_filter, requires_grad=False)\n",
    "        self.iwt_filter = nn.Parameter(self.iwt_filter, requires_grad=False)\n",
    "\n",
    "        self.base_conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels, bias=bias)\n",
    "        self.base_scale = _ScaleModule([1,in_channels,1,1])\n",
    "\n",
    "        self.wavelet_convs = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels*4, in_channels*4, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels*4, bias=False) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "        self.wavelet_scale = nn.ModuleList(\n",
    "            [_ScaleModule([1,in_channels*4,1,1], init_scale=0.1) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "\n",
    "        if self.stride > 1:\n",
    "            self.do_stride = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.do_stride = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_ll_in_levels = []\n",
    "        x_h_in_levels = []\n",
    "        shapes_in_levels = []\n",
    "\n",
    "        curr_x_ll = x\n",
    "\n",
    "        for i in range(self.wt_levels):\n",
    "            curr_shape = curr_x_ll.shape\n",
    "            shapes_in_levels.append(curr_shape)\n",
    "            if (curr_shape[2] % 2 > 0) or (curr_shape[3] % 2 > 0):\n",
    "                curr_pads = (0, curr_shape[3] % 2, 0, curr_shape[2] % 2)\n",
    "                curr_x_ll = F.pad(curr_x_ll, curr_pads)\n",
    "\n",
    "            curr_x = wavelet.wavelet_transform(curr_x_ll, self.wt_filter)\n",
    "            curr_x_ll = curr_x[:,:,0,:,:]\n",
    "            \n",
    "            shape_x = curr_x.shape\n",
    "            curr_x_tag = curr_x.reshape(shape_x[0], shape_x[1] * 4, shape_x[3], shape_x[4])\n",
    "            curr_x_tag = self.wavelet_scale[i](self.wavelet_convs[i](curr_x_tag))\n",
    "            curr_x_tag = curr_x_tag.reshape(shape_x)\n",
    "\n",
    "            x_ll_in_levels.append(curr_x_tag[:,:,0,:,:])\n",
    "            x_h_in_levels.append(curr_x_tag[:,:,1:4,:,:])\n",
    "\n",
    "        next_x_ll = 0\n",
    "\n",
    "        for i in range(self.wt_levels-1, -1, -1):\n",
    "            curr_x_ll = x_ll_in_levels.pop()\n",
    "            curr_x_h = x_h_in_levels.pop()\n",
    "            curr_shape = shapes_in_levels.pop()\n",
    "\n",
    "            curr_x_ll = curr_x_ll + next_x_ll\n",
    "\n",
    "            curr_x = torch.cat([curr_x_ll.unsqueeze(2), curr_x_h], dim=2)\n",
    "            next_x_ll = wavelet.inverse_wavelet_transform(curr_x, self.iwt_filter)\n",
    "\n",
    "            next_x_ll = next_x_ll[:, :, :curr_shape[2], :curr_shape[3]]\n",
    "\n",
    "        x_tag = next_x_ll\n",
    "        assert len(x_ll_in_levels) == 0\n",
    "        \n",
    "        x = self.base_scale(self.base_conv(x))\n",
    "        x = x + x_tag\n",
    "        \n",
    "        if self.do_stride is not None:\n",
    "            x = self.do_stride(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Spatial_layer(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(Spatial_layer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        out = self.sigmoid(x)*identity\n",
    "        return out\n",
    "    \n",
    "class Channel_layer(nn.Module):\n",
    "    \"\"\"Constructs a channel layer.\n",
    "    Args:k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, k_size=3):\n",
    "        super(Channel_layer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x)\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        out = x * y.expand_as(x)\n",
    "        return out\n",
    "    \n",
    "class Temporal_layer(nn.Module):\n",
    "    \"\"\"Constructs a Temporal layer.\n",
    "    Args:k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, num_T=16):\n",
    "        super(Temporal_layer, self).__init__()\n",
    "\n",
    "        self.sa_layer = Spatial_layer()\n",
    "        self.ch_layer = Channel_layer()\n",
    "\n",
    "        self.conv = nn.Conv2d(2*num_T, 1*num_T, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_s = self.sa_layer(x)\n",
    "        y_c = self.ch_layer(x)\n",
    "        y_t = torch.cat([y_s, y_c], dim=1)\n",
    "        y_t = self.conv(y_t)  \n",
    "\n",
    "        out = self.sigmoid(y_t)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=40, kernel_size=16, D=2, pooling_size1=2, pooling_size2=2, dropout_rate=0.3, number_channel=52, emb_size=40, sampling_points=100):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(2, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            # nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            # # Temporal_layer(num_T=f2),\n",
    "            # nn.BatchNorm2d(f2),\n",
    "            # nn.ELU(),\n",
    "            # # average pooling 1\n",
    "            # # nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            # nn.Dropout(dropout_rate),\n",
    "            # # spatial conv\n",
    "            # nn.Conv2d(f2, f2, (1, 32), padding='same', bias=False), \n",
    "            # # Temporal_layer(num_T=f2),\n",
    "            # nn.BatchNorm2d(f2),\n",
    "            # nn.ELU(),\n",
    "            # # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            # # nn.AvgPool2d((1, pooling_size2)),\n",
    "            # nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "        # self.cnn_module = nn.Conv2d(2, f1, (1, kernel_size), (1, 1), padding='same', bias=False) # [batch, 22, 1000] \n",
    "        self.Conv1 = nn.Conv2d(f1, f1, (1,1), padding='same')\n",
    "        self.Conv2 = nn.Conv2d(f1, f1, (1, (sampling_points//4)),  padding='same')\n",
    "        self.Conv3 = nn.Conv2d(f1, f1, (1, (sampling_points//8)),  padding='same')\n",
    "        self.Conv4 = nn.Conv2d(f1, f1, (1, (sampling_points//16)), padding='same')\n",
    "        self.InputMaxPooling = nn.MaxPool2d((1,1))\n",
    "        self.Conv1_2 = nn.Conv2d(f1, f1, (1,1), padding='same')\n",
    "        self.bottleneck = nn.Sequential( \n",
    "            nn.Conv2d(4 * f1, f1, (1, 1)),  # 输入通道=4f1 (4个特征图)\n",
    "            nn.BatchNorm2d(f1)\n",
    "        )\n",
    "        self.Conv1_3 = nn.Conv2d(f1, f1, (1,1), padding='same')\n",
    "        \n",
    "        self.BN = nn.BatchNorm2d(f1)\n",
    "\n",
    "\n",
    "        self.pre_att = Temporal_layer(num_T=f2)\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x = x.unsqueeze(1)\n",
    "        b, _, _, _ = x.shape # b, 2, 52, 140\n",
    "        if x.shape[1]!=2:\n",
    "            print(x.shape)\n",
    "            assert 'ArithmeticError'\n",
    "        x = self.cnn_module(x)\n",
    "        # print(\"x after cnn: \", x.shape) #(128, 40, 1, 2)\n",
    "        c1 = self.Conv1(x)\n",
    "        xmp = self.InputMaxPooling(x)\n",
    "        c2 = self.Conv2(c1)\n",
    "        c3 = self.Conv3(c1)\n",
    "        c4 = self.Conv4(c1)\n",
    "        c12 = self.Conv1_2(xmp)\n",
    "        xc = torch.cat([c2,c3,c4,c12], dim=1)\n",
    "        xc = self.bottleneck(xc)\n",
    "        x13 = self.Conv1_3(xc)\n",
    "        y = self.BN(x13)\n",
    "\n",
    "        y = self.projection(y) # (128, 2, 40)\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer_wavelet(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN_wavelet(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "#             TransformerEncoder(heads, depth, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "#             TransformerEncoder(heads, depth, emb_size),\n",
    "        )\n",
    "    \n",
    "\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        # x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    from: https://github.com/yaohungt/Multimodal-Transformer\n",
    "    Multi-headed attention.\n",
    "    See \"Attention Is All You Need\" for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.,\n",
    "                 bias=True, add_bias_kv=False, add_zero_attn=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.in_proj_weight = nn.Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
    "        self.register_parameter('in_proj_bias', None)\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.Tensor(3 * embed_dim))\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Self-attention can be implemented by passing in the same arguments for\n",
    "        query, key and value. Timesteps can be masked by supplying a T x T mask in the\n",
    "        `attn_mask` argument. Padding elements can be excluded from\n",
    "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
    "        batch x src_len, where padding elements are indicated by 1s.\n",
    "        \"\"\"\n",
    "        qkv_same = query.data_ptr() == key.data_ptr() == value.data_ptr()\n",
    "        kv_same = key.data_ptr() == value.data_ptr()\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "        assert key.size() == value.size()\n",
    "\n",
    "        aved_state = None\n",
    "\n",
    "        if qkv_same:\n",
    "            # self-attention\n",
    "            q, k, v = self.in_proj_qkv(query)\n",
    "        elif kv_same:\n",
    "            # encoder-decoder attention\n",
    "            q = self.in_proj_q(query)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = v = None\n",
    "            else:\n",
    "                k, v = self.in_proj_kv(key)\n",
    "        else:\n",
    "            q = self.in_proj_q(query)\n",
    "            k = self.in_proj_k(key)\n",
    "            v = self.in_proj_v(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        if self.bias_k is not None:\n",
    "            assert self.bias_v is not None\n",
    "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
    "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "        \n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            try:\n",
    "                attn_weights += attn_mask.unsqueeze(0)\n",
    "            except:\n",
    "                print(attn_weights.shape)\n",
    "                print(attn_mask.unsqueeze(0).shape)\n",
    "                assert False\n",
    "                \n",
    "        attn_weights = F.softmax(attn_weights.float(), dim=-1).type_as(attn_weights)\n",
    "        # attn_weights = F.relu(attn_weights)\n",
    "        # attn_weights = attn_weights / torch.max(attn_weights)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        attn = torch.bmm(attn_weights, v)\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "\n",
    "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "\n",
    "        # average attention weights over heads\n",
    "        attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "        attn_weights = attn_weights.sum(dim=1) / self.num_heads\n",
    "        return attn, attn_weights\n",
    "\n",
    "    def in_proj_qkv(self, query):\n",
    "        return self._in_proj(query).chunk(3, dim=-1)\n",
    "\n",
    "    def in_proj_kv(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim).chunk(2, dim=-1)\n",
    "\n",
    "    def in_proj_q(self, query, **kwargs):\n",
    "        return self._in_proj(query, end=self.embed_dim, **kwargs)\n",
    "\n",
    "    def in_proj_k(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
    "\n",
    "    def in_proj_v(self, value):\n",
    "        return self._in_proj(value, start=2 * self.embed_dim)\n",
    "\n",
    "    def _in_proj(self, input, start=0, end=None, **kwargs):\n",
    "        weight = kwargs.get('weight', self.in_proj_weight)\n",
    "        bias = kwargs.get('bias', self.in_proj_bias)\n",
    "        weight = weight[start:end, :]\n",
    "        if bias is not None:\n",
    "            bias = bias[start:end]\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer block.\n",
    "    In the original paper each operation (multi-head attention or FFN) is\n",
    "    postprocessed with: `dropout -> add residual -> layernorm`. In the\n",
    "    tensor2tensor code they suggest that learning is more robust when\n",
    "    preprocessing each layer with layernorm and postprocessing with:\n",
    "    `dropout -> add residual`. We default to the approach in the paper, but the\n",
    "    tensor2tensor approach can be enabled by setting\n",
    "    *args.encoder_normalize_before* to ``True``.\n",
    "    Args:\n",
    "        embed_dim: Embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=4, attn_dropout=0.1, relu_dropout=0.1, res_dropout=0.1,\n",
    "                 attn_mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            attn_dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "        self.relu_dropout = relu_dropout\n",
    "        self.res_dropout = res_dropout\n",
    "        self.normalize_before = True\n",
    "\n",
    "        self.fc1 = nn.Linear(self.embed_dim, 4*self.embed_dim)   # The \"Add & Norm\" part in the paper\n",
    "        self.fc2 = nn.Linear(4*self.embed_dim, self.embed_dim)\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(self.embed_dim) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, x_k=None, x_v=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
    "            x_k (Tensor): same as x\n",
    "            x_v (Tensor): same as x\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(0, x, before=True)\n",
    "        mask = None\n",
    "        if x_k is None and x_v is None:\n",
    "            x, _ = self.self_attn(query=x, key=x, value=x, attn_mask=mask)\n",
    "        else:\n",
    "            x_k = self.maybe_layer_norm(0, x_k, before=True)\n",
    "            x_v = self.maybe_layer_norm(0, x_v, before=True) \n",
    "            x, _ = self.self_attn(query=x, key=x_k, value=x_v, attn_mask=mask)\n",
    "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(0, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(1, x, before=True)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(1, x, after=True)\n",
    "        return x\n",
    "\n",
    "    def maybe_layer_norm(self, i, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return self.layer_norms[i](x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.features = None\n",
    "\n",
    "    def save_features(self, features):\n",
    "        self.features = features.detach().cpu().numpy()  # 转换为numpy方便后续t-SNE\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "\n",
    "\n",
    "class fNIRS_TTT_LM(nn.Module):\n",
    "    def __init__(self, n_class, sampling_point, dim, depth, heads, mlp_dim, pool='cls', \n",
    "                 dim_head=64, dropout=0., emb_dropout=0.1,intermediate_size=4, dataset=\"A\",\n",
    "                 mini_batch_size=16, device='cpu', batch_size=128):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.bs = batch_size\n",
    "        kernel_time = 30\n",
    "        match dataset:\n",
    "            case \"A\":\n",
    "                patch_num_head = 2\n",
    "                input_ch = 52\n",
    "                patch_channel_size = 5\n",
    "                inner_channels = 8\n",
    "\n",
    "            case \"B\":\n",
    "                patch_num_head = 4\n",
    "                input_ch = 36\n",
    "                patch_channel_size = 5\n",
    "                inner_channels = 8\n",
    "\n",
    "            case \"C\":\n",
    "                patch_num_head = 8\n",
    "                input_ch = 20\n",
    "                patch_channel_size = 2\n",
    "                inner_channels = 8\n",
    "\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=inner_channels, kernel_size=(patch_channel_size, kernel_time), padding=\"same\"),\n",
    "            # nn.Conv2d(in_channels=2, out_channels=4, kernel_size=(2, 30), padding=\"same\"),\n",
    "            Rearrange('b c h w  -> b w (c h)'), #bs, feature_dim, ch, time -> bs, time, feature_dim*ch\n",
    "            nn.Linear(inner_channels*input_ch, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "            )\n",
    "\n",
    "        self.to_channel_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=inner_channels, kernel_size=(1, kernel_time), padding=\"same\"),\n",
    "            Rearrange('b c h w  -> b w (c h)'), #bs, feature_dim, ch, time -> bs, time, feature_dim*ch\n",
    "            nn.Linear(inner_channels*input_ch, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "            )\n",
    "\n",
    "        self.pos_embedding_patch = nn.Parameter(torch.randn(1, sampling_point, dim))\n",
    "        self.cls_token_patch = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout_patch = nn.Dropout(emb_dropout)\n",
    "        self.layernormalization = nn.LayerNorm(dim)\n",
    "        self.cross_transformer = TransformerEncoderLayer(dim, heads)\n",
    "        self.feature_extractor = FeatureExtractor()  # 初始化特征提取器\n",
    "\n",
    "        config_p = ttt.TTTConfig(\n",
    "                                hidden_size=sampling_point,           # 隐藏层大小\n",
    "                                intermediate_size=sampling_point*intermediate_size,    # MLP中间层的大小，可以设置为hidden_size的倍数\n",
    "                                num_hidden_layers=1,      # 隐藏层的数量\n",
    "                                num_attention_heads=patch_num_head,    # 注意力头的数量\n",
    "                                rms_norm_eps=1e-6,        # RMS归一化epsilon值\n",
    "                                mini_batch_size=mini_batch_size )\n",
    "        self.ttt_PreNorm_patch = nn.LayerNorm(dim)\n",
    "        self.tttMLP = ttt.TTTMLP(config_p, layer_idx=0).to(device)\n",
    "        self.patch_cache = ttt.TTTCache_MK2(self.tttMLP, batch_size, self.tttMLP.config.mini_batch_size).to(device)\n",
    "        \n",
    "        self.pos_embedding_channel = nn.Parameter(torch.randn(1, sampling_point, dim))\n",
    "        self.cls_token_channel = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout_channel = nn.Dropout(emb_dropout)\n",
    "\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, n_class))\n",
    "        \n",
    "    def fulfill(source, aim):\n",
    "        tensor1 = aim\n",
    "        tensor2 = source\n",
    "        padded_tensor1 = torch.cat([tensor2, torch.zeros((tensor1.shape[0] - tensor2.shape[0]), \n",
    "                                                        tensor2.shape[1], \n",
    "                                                        tensor2.shape[2],\n",
    "                                                        tensor2.shape[3]).to(tensor1.device)], dim=0)\n",
    "        return(padded_tensor1)\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        n_samples = img.shape[0]\n",
    "        if n_samples != self.bs:\n",
    "            img = torch.cat([img, torch.zeros((self.bs - n_samples, \n",
    "                                               img.shape[1],\n",
    "                                               img.shape[2],\n",
    "                                               img.shape[3])).to(self.device)], dim=0)\n",
    "        x = self.to_patch_embedding(img)\n",
    "        x2 = self.to_channel_embedding(img.squeeze())\n",
    "\n",
    "        # pos embedding\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding_patch[:, :(n + 1)]\n",
    "        x = self.dropout_patch(x)\n",
    "        b, n, _ = x2.shape\n",
    "        x2 += self.pos_embedding_channel[:, :(n + 1)]\n",
    "        x2 = self.dropout_channel(x2)\n",
    "        \n",
    "        # #cross attn\n",
    "        # cr_s1 = x\n",
    "        # cr_s2 = x2\n",
    "        # x = self.cross_transformer(cr_s1, cr_s2, cr_s2)\n",
    "        # concat\n",
    "        x = torch.cat([x,x2],dim=2)\n",
    "\n",
    "        #ttt\n",
    "        x = self.ttt_PreNorm_patch(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        patch_position_ids = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0],1).to(self.device)\n",
    "        x = self.tttMLP(x, position_ids=patch_position_ids ,cache_params=self.patch_cache)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "        x_cross = self.to_latent(x)\n",
    "\n",
    "        # 保存特征\n",
    "        self.feature_extractor.save_features(x_cross)\n",
    "\n",
    "        def get_saved_features(self):\n",
    "            return self.feature_extractor.get_features()\n",
    "        \n",
    "        return self.mlp_head(x_cross)[:n_samples]\n",
    "\n",
    "\n",
    "class CTNet(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=16,\n",
    "                 depth=6, \n",
    "                 eeg1_f1 = 40,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.1,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 240,\n",
    "                 n_class = 2,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = n_class, eeg1_number_channel\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        # cnn部分\n",
    "        # self.Conv1 = nn.Conv2d(2, eeg1_f1, (1,1), padding='same')\n",
    "        # self.Conv2 = nn.Conv2d(eeg1_f1, eeg1_f1, (1, (sampling_points//4)),  padding='same')\n",
    "        # self.Conv3 = nn.Conv2d(eeg1_f1, eeg1_f1, (1, (sampling_points//8)),  padding='same')\n",
    "        # self.Conv4 = nn.Conv2d(eeg1_f1, eeg1_f1, (1, (sampling_points//16)), padding='same')\n",
    "        # self.InputMaxPooling = nn.MaxPool2d((1,1))\n",
    "        # self.Conv1_2 = nn.Conv2d(2, eeg1_f1, (1,1), padding='same')\n",
    "        # self.bottleneck = nn.Sequential( \n",
    "        #     nn.Conv2d(4 * eeg1_f1, eeg1_f1, (1, 1)),  # 输入通道=4f1 (4个特征图)\n",
    "        #     nn.BatchNorm2d(eeg1_f1)\n",
    "        # )\n",
    "        # self.Conv1_3 = nn.Conv2d(eeg1_f1, eeg1_f1, (1,1), padding='same')\n",
    "        # self.BN = nn.BatchNorm2d(eeg1_f1)\n",
    "\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape) (128, 2, 52, 140)\n",
    "        cnn = self.cnn(x)\n",
    "        # cnn2 = self.cnn_wavelet(x)\n",
    "        #print(cnn.shape)\n",
    "        #print(cnn2.shape)\n",
    "        # add label \n",
    "        # cnn = cnn * math.sqrt(self.emb_size)\n",
    "        # cnn = self.position(cnn) (128, 140, 40)\n",
    "        \n",
    "        trans = self.trans(cnn) #(128, 140, 40)\n",
    "\n",
    "        # cnn_fusion = self.cross_attention(cnn, cnn2, cnn2)\n",
    "        # cnn_fusion = self.flatten(cnn_fusion)\n",
    "        \n",
    "        # features = cnn2 + trans + cnn\n",
    "        # features = trans + cnn_fusion\n",
    "        features = trans + cnn\n",
    "\n",
    "        # features = cnn\n",
    "        # features = self.cross_attention(query=cnn, key=trans, value=trans)\n",
    "        # print(features.shape)\n",
    "        out = self.classification(self.flatten(features))\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a92ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      " 5\n",
      "torch.Size([1800, 2, 20, 256])\n",
      "torch.Size([1800])\n",
      "torch.Size([450, 2, 20, 256])\n",
      "torch.Size([450])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 GiB. GPU 0 has a total capacty of 79.32 GiB of which 10.14 GiB is free. Process 111527 has 3.60 GiB memory in use. Process 12272 has 61.14 GiB memory in use. Process 130920 has 4.37 GiB memory in use. Of the allocated memory 2.68 GiB is allocated by PyTorch, and 108.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     70\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Piecewise decay flooding. b is flooding level, b = 0 means no flooding\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 838\u001b[0m, in \u001b[0;36mCTNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    830\u001b[0m cnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# cnn2 = self.cnn_wavelet(x)\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;66;03m#print(cnn.shape)\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m#print(cnn2.shape)\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# add label \u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# cnn = cnn * math.sqrt(self.emb_size)\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# cnn = self.position(cnn) (128, 140, 40)\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(128, 140, 40)\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# cnn_fusion = self.cross_attention(cnn, cnn2, cnn2)\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# cnn_fusion = self.flatten(cnn_fusion)\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# features = cnn2 + trans + cnn\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# features = trans + cnn_fusion\u001b[39;00m\n\u001b[1;32m    845\u001b[0m features \u001b[38;5;241m=\u001b[39m trans \u001b[38;5;241m+\u001b[39m cnn\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 293\u001b[0m, in \u001b[0;36mResidualAdd.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 293\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(res)\u001b[38;5;241m+\u001b[39mx_input)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 244\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m keys \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys(x), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m    243\u001b[0m values \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues(x), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m--> 244\u001b[0m energy \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbhqd, bhkd -> bhqk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mmin\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 GiB. GPU 0 has a total capacty of 79.32 GiB of which 10.14 GiB is free. Process 111527 has 3.60 GiB memory in use. Process 12272 has 61.14 GiB memory in use. Process 130920 has 4.37 GiB memory in use. Of the allocated memory 2.68 GiB is allocated by PyTorch, and 108.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in rkf.split(feature):\n",
    "    n_runs += 1\n",
    "    print('======================================\\n', n_runs)\n",
    "\n",
    "    X_train = feature[train_index]\n",
    "    y_train = label[train_index]\n",
    "    X_test = feature[test_index]\n",
    "    y_test = label[test_index]\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 2, channels, -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 2, channels, -1))\n",
    "\n",
    "    train_set = Dataset(X_train, y_train, transform=True)\n",
    "    test_set = Dataset(X_test, y_test, transform=True)\n",
    "    ########### fix seed ###########\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    ################################\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    # sample = train_set[0]\n",
    "    # in_shape = sample.shape\n",
    "    # -------------------------------------------------------------------------------------------------------------------- #\n",
    "    device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    match dataset[dataset_id]:\n",
    "        case \"A\":\n",
    "            n_class = 2\n",
    "            sampling_point = 140\n",
    "            h = 52\n",
    "            ct_f = int(5600)\n",
    "        case \"B\":\n",
    "            n_class = 2\n",
    "            sampling_point = 200\n",
    "            h = 36\n",
    "            ct_f = int(8000)\n",
    "        case \"C\":\n",
    "            n_class = 3\n",
    "            sampling_point = 256\n",
    "            h = 20\n",
    "            ct_f = int(sampling_point*h*2)\n",
    "\n",
    "    # net = CTNet(heads=4, emb_size=40,depth=6, eeg1_f1=40, \n",
    "    #                     eeg1_D=2,eeg1_kernel_size=32, eeg1_pooling_size1=2, eeg1_pooling_size2=2,\n",
    "    #                     eeg1_dropout_rate=0.1,eeg1_number_channel=h,flatten_eeg1=ct_f,n_class=n_class).to(device) # A 80 B 120 C 160\n",
    "\n",
    "    net = fNIRS_TTT_LM(n_class=n_class, sampling_point=sampling_point,\n",
    "                             dim=64, depth=6, heads=8, mlp_dim=64, device=device,\n",
    "                             batch_size=batch_size, dataset=dataset[dataset_id]).to(device)\n",
    "    criterion = LabelSmoothing(0.1)\n",
    "    optimizer = torch.optim.AdamW(net.parameters())\n",
    "    lrStep = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "    # -------------------------------------------------------------------------------------------------------------------- #\n",
    "    test_max_acc = 0\n",
    "\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    break\n",
    "\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    train_running_acc = 0\n",
    "    total = 0\n",
    "    loss_steps = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "\n",
    "        # Piecewise decay flooding. b is flooding level, b = 0 means no flooding\n",
    "        if epoch < 30:\n",
    "            b = flooding_level[0]\n",
    "        elif epoch < 50:\n",
    "            b = flooding_level[1]\n",
    "        else:\n",
    "            b = flooding_level[2]\n",
    "\n",
    "        # flooding\n",
    "        loss = (loss - b).abs() + b\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_steps.append(loss.item())\n",
    "        total += labels.shape[0]\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        train_running_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    train_running_loss = float(np.mean(loss_steps))\n",
    "    train_running_acc = 100 * train_running_acc / total\n",
    "    # 将训练损失和准确率保存到对应列表\n",
    "    train_loss_history.append(train_running_loss)\n",
    "    train_acc_history.append(train_running_acc)\n",
    "    print('[%d, %d] Train loss: %0.4f' % (n_runs, epoch, train_running_loss))\n",
    "    print('[%d, %d] Train acc: %0.3f%%' % (n_runs, epoch, train_running_acc))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------- #\n",
    "    net.eval()\n",
    "    test_running_acc = 0\n",
    "    total = 0\n",
    "    loss_steps = []\n",
    "    y_label = y_pred = None\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            loss_steps.append(loss.item())\n",
    "            total += labels.shape[0]\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            test_running_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        test_running_acc = 100 * test_running_acc / total\n",
    "        test_running_loss = float(np.mean(loss_steps))\n",
    "        test_loss_history.append(test_running_loss)\n",
    "        test_acc_history.append(test_running_acc)\n",
    "        print('     [%d, %d] Test loss: %0.4f' % (n_runs, epoch, test_running_loss))\n",
    "        print('     [%d, %d] Test acc: %0.3f%%' % (n_runs, epoch, test_running_acc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
